# use { #chapy.use }

`use(provider='', **kwargs)`

Specify which LLM you will use

## Details

Indicate which LLM and model you would like to use during your Python session.
At this time Ollama, and OpenAI, models are supported.

Passing no `provider` will automatically prompt you to select a provider and a
model. For Ollama, the models will be those currently installed in your
laptop.

## Parameters

| Name       | Type   | Description                                                                    | Default   |
|------------|--------|--------------------------------------------------------------------------------|-----------|
| `provider` |        | Name of the LLM provider you wish to use. Currently, only 'olama' is supported | `''`      |
| `**kwargs` |        | Arguments to override the defaults. Such as the 'model', amd 'system_msg'      | `{}`      |

## Examples

```python
import chapy

chapy.use("ollama")

chapy.use("openai", model = "gpt4o")
```